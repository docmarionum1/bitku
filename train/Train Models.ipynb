{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary of words/syllables\n",
    "\n",
    "# Standard Dict\n",
    "WORDS = {}\n",
    "dictionary_path = Path('dictionaries')\n",
    "with (dictionary_path / 'cmudict.dict.txt').open('r') as f:\n",
    "    for line in f.readlines():\n",
    "        word, phonemes = line.strip().split(' ', 1)\n",
    "        word = re.match(r'([^\\(\\)]*)(\\(\\d\\))*', word).groups()[0]\n",
    "        phonemes = phonemes.split(' ')\n",
    "        syllables = sum([re.match(r'.*\\d', p) is not None for p in phonemes])\n",
    "        if word not in WORDS:\n",
    "            WORDS[word] = []\n",
    "        WORDS[word].append({\n",
    "            'phonemes': phonemes,\n",
    "            'syllables': syllables\n",
    "        })\n",
    "        \n",
    "# Load custom phonemes\n",
    "CUSTOM_WORDS = {}\n",
    "vowels = ['AA', 'AE', 'AH', 'AO', 'AW', 'AX', 'AXR', 'AY', 'EH', 'ER', 'EY', 'IH', 'IX', 'IY', 'OW', 'OY', 'UH', 'UW', 'UX']\n",
    "with (dictionary_path / 'custom.dict.txt').open('r') as f:\n",
    "    for line in f.readlines():\n",
    "        try:\n",
    "            word, phonemes = line.strip().split('\\t', 1)\n",
    "        except:\n",
    "            continue\n",
    "        word = re.match(r'([^\\(\\)]*)(\\(\\d\\))*', word).groups()[0].lower()\n",
    "        phonemes = phonemes.split(' ')\n",
    "        syllables = sum([(p in vowels) for p in phonemes])\n",
    "        \n",
    "        if word not in CUSTOM_WORDS:\n",
    "            CUSTOM_WORDS[word] = []\n",
    "        CUSTOM_WORDS[word].append({\n",
    "            'phonemes': phonemes,\n",
    "            'syllables': syllables\n",
    "        })\n",
    "        \n",
    "WORDS.update(CUSTOM_WORDS)\n",
    "\n",
    "# Simplify for this project\n",
    "WORDS = {w: WORDS[w][0]['syllables'] for w in WORDS}\n",
    "WORDS[\"\\n\"] = 0\n",
    "\n",
    "# Corrections for captialization\n",
    "CORRECTIONS = {\n",
    "    \"i\": \"I\",\n",
    "    \"i'm\": \"I'm\",\n",
    "    \"i've\": \"I've\",\n",
    "    \"i'll\": \"I'll\",\n",
    "    \"i'd\": \"I'd\",\n",
    "    \"jesus\": \"Jesus\",\n",
    "    \"trump\": \"Trump\",\n",
    "    \"twitter\": \"Twitter\",\n",
    "    \"facebook\": \"Facebook\",\n",
    "    \"snapchat\": \"Snapchat\",\n",
    "    \"john\": \"John\",\n",
    "    \"america\": \"America\",\n",
    "    \"valentine's\": \"Valentine's\",\n",
    "    \"halloween\": \"Halloween\",\n",
    "    \"etc\": \"etc.\",\n",
    "    \"god\": \"God\",\n",
    "    \"god's\": \"God's\",\n",
    "    \"youtube\": \"YouTube\"\n",
    "}\n",
    "\n",
    "for name in calendar.month_name:\n",
    "    CORRECTIONS[name.lower()] = name\n",
    "\n",
    "for name in calendar.day_name:\n",
    "    CORRECTIONS[name.lower()] = name\n",
    "\n",
    "del CORRECTIONS[\"\"]\n",
    "\n",
    "for k, v in CORRECTIONS.items():\n",
    "    WORDS[v] = WORDS[k]\n",
    "    del WORDS[k]\n",
    "\n",
    "# Remove some words from the dictionary\n",
    "for word in [\n",
    "    \"ur\",\n",
    "    \"tho\",\n",
    "    \"ion\",\n",
    "    \"ill\", # ambiguous usage\n",
    "    \"cant\",\n",
    "    \"mr\",\n",
    "    \"mrs\",\n",
    "    \"st\",\n",
    "    \"ii\",\n",
    "    \n",
    "    # Avoid _potentially_ offensive haikus. These words aren't necessarily offensive, but the corpus has a lot of\n",
    "    # offensive uses of them, so best to avoid it entirely.\n",
    "    \"gay\", \n",
    "]:\n",
    "    del WORDS[word]\n",
    "    \n",
    "for letter in \"bcdefghjklmnpqrstvwxyz\":\n",
    "    del WORDS[letter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"syllables\" has 3 syllables\n"
     ]
    }
   ],
   "source": [
    "# The result is a dictionary mapping words to the number of syllables they have\n",
    "print(f'\"syllables\" has {WORDS[\"syllables\"]} syllables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012916\n"
     ]
    }
   ],
   "source": [
    "# Load the corpii of corpora of training data and transform them into a list of transitions from words and word pairs\n",
    "# into the next word. These corpora aren't specifically haikus, just text; this is because we don't have enough\n",
    "# haikus to work with.\n",
    "\n",
    "corpus = []\n",
    "regex = re.compile(r\"[^.a-z\\s'-]\")\n",
    "\n",
    "# Text files from Gutenberg\n",
    "CORPUS_DIR = Path('corpus')\n",
    "for path in CORPUS_DIR.glob('*.txt'):\n",
    "    with open(str(path)) as f:\n",
    "        corpus = corpus + [\n",
    "            i.split() for i in re.split(\n",
    "                r'[\\.!\\?;]',\n",
    "                regex.sub('',\n",
    "                    f.read().lower().replace(\"--\", \" \")#.replace(\",\", \"\").replace(\"--\", \" \").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "# Brown Corpus\n",
    "from nltk.corpus import brown\n",
    "corpus += [re.split(r'[\\.!\\?;\\s]+', regex.sub('', ' '.join(s).lower().replace(\"''\", \"\"))) for s in brown.sents()]\n",
    "\n",
    "# Gutenberg Poetry Corpus\n",
    "all_lines = []\n",
    "for line in gzip.open(\"corpus/gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    all_lines.append(json.loads(line.strip()))\n",
    "    \n",
    "big_poem = \"\\n\".join([line['s'] for line in all_lines])\n",
    "corpus += [\n",
    "    i.split() for i in re.split(\n",
    "        r'[\\.!\\?;]',\n",
    "        regex.sub('',\n",
    "            big_poem.lower().replace(\"--\", \" \")#.replace(\",\", \"\").replace(\"--\", \" \").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# Functions for processing the corpora into transitions \n",
    "def correct_sentence(sentence):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        if word in CORRECTIONS:\n",
    "            word = CORRECTIONS[word]\n",
    "            \n",
    "        if word not in WORDS:\n",
    "            raise KeyError\n",
    "            \n",
    "        new_words.append(word)\n",
    "        \n",
    "    return new_words\n",
    "\n",
    "def haikuify(sentence):\n",
    "    try:\n",
    "        sentence = correct_sentence(sentence)\n",
    "    except KeyError:\n",
    "        return []\n",
    "    \n",
    "    total_syllables = 0\n",
    "    for word in sentence:\n",
    "        total_syllables += WORDS[word]\n",
    "        \n",
    "    if total_syllables < 10 or total_syllables > 21:\n",
    "        return []\n",
    "    \n",
    "    targets = [total_syllables * 5/17, total_syllables * 12/17, total_syllables]\n",
    "    target = total_syllables * 5/17\n",
    "    new_sentence = []\n",
    "    syllables = 0\n",
    "    while syllables < target:\n",
    "        word = sentence.pop(0)\n",
    "        new_sentence.append(word)\n",
    "        syllables += WORDS[word]\n",
    "    new_sentence.append(\"\\n\")\n",
    "    \n",
    "    target = total_syllables * 12/17\n",
    "    while syllables < target:\n",
    "        word = sentence.pop(0)\n",
    "        new_sentence.append(word)\n",
    "        syllables += WORDS[word]\n",
    "        \n",
    "    return new_sentence + [\"\\n\"] + sentence + [\"END\"]\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    if not sentence:\n",
    "        return []\n",
    "    syllable_count = 0\n",
    "    line_num = 0\n",
    "    new_rows = [\n",
    "        (0, 0, \"START\", sentence[0], False)\n",
    "    ]\n",
    "    for i in range(len(sentence) - 1):\n",
    "        word = sentence[i]\n",
    "        next_word = sentence[i + 1]\n",
    "        next_next_word = sentence[i + 2] if (i + 2) < len(sentence) else None\n",
    "        \n",
    "        \n",
    "        if word in CORRECTIONS:\n",
    "            word = CORRECTIONS[word]\n",
    "            \n",
    "        if next_word in CORRECTIONS:\n",
    "            next_word = CORRECTIONS[next_word]\n",
    "            \n",
    "        if next_next_word in CORRECTIONS:\n",
    "            next_next_word = CORRECTIONS[next_next_word]     \n",
    "        \n",
    "        if word not in WORDS:\n",
    "            return []\n",
    "        \n",
    "        syllable_count += WORDS[word]\n",
    "        \n",
    "        new_rows.append((line_num, syllable_count, word, next_word, i == len(sentence) - 3))\n",
    "        if next_next_word:\n",
    "            new_rows.append((line_num, syllable_count, word + \" \" + next_word, next_next_word, i == len(sentence) - 4))\n",
    "            \n",
    "        if word == \"\\n\":\n",
    "            line_num += 1\n",
    "    \n",
    "    if (syllable_count < 10) or (syllable_count > 23):\n",
    "        return []\n",
    "    \n",
    "    return new_rows\n",
    "\n",
    "transitions = []\n",
    "for sentence in corpus:\n",
    "    sentence = haikuify(sentence)\n",
    "    if sentence:\n",
    "        transitions += process_sentence(sentence)\n",
    "        \n",
    "print(len(transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6455634"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also load a CSV of preprocessed haikus\n",
    "haiku_df = pd.read_csv('corpus/haikus.csv')\n",
    "haiku_df = haiku_df.drop_duplicates(subset=[\"0\", \"1\", \"2\"])\n",
    "\n",
    "corpus = []\n",
    "regex = re.compile(r\"[^.a-z\\s'-]\")\n",
    "\n",
    "for i, row in haiku_df.iterrows():\n",
    "    try:\n",
    "        if (row['0_syllables'] + row['1_syllables'] + row['2_syllables']) <= 17:\n",
    "            sentence = row[\"0\"] + \" \\n \" + row[\"1\"] + \" \\n \" + row[\"2\"]\n",
    "            sentence = regex.sub('', sentence.lower().replace(\"--\", \" \").replace(\" - \", \" \"))\n",
    "            sentence = sentence + \" END\"\n",
    "            corpus.append([w for w in sentence.split(\" \") if w])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for sentence in corpus:\n",
    "    transitions += process_sentence(sentence)\n",
    "    \n",
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490398\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_num</th>\n",
       "      <th>syllable</th>\n",
       "      <th>word</th>\n",
       "      <th>next_word</th>\n",
       "      <th>end</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>I</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>I</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>I</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>I</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>I</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_num  syllable   word next_word    end  0\n",
       "0         0         0  START         I  False  1\n",
       "1         0         0  START         I  False  1\n",
       "2         0         0  START         I  False  1\n",
       "3         0         0  START         I  False  1\n",
       "4         0         0  START         I  False  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe from the transitions list\n",
    "transitions_df = pd.DataFrame(transitions, columns=[\"line_num\", \"syllable\", \"word\", \"next_word\", \"end\"])\n",
    "transitions_df[0] = 1\n",
    "\n",
    "# Get rid of pairs that only occur once\n",
    "pair_counts = (transitions_df.groupby([\"word\", \"next_word\"]).count()[0] > 1).reset_index()\n",
    "pair_counts = pair_counts[pair_counts[0]][[\"word\", \"next_word\"]]\n",
    "transitions_df = transitions_df.merge(pair_counts, on=[\"word\", \"next_word\"])\n",
    "\n",
    "print(len(transitions_df))\n",
    "\n",
    "transitions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490398\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of the 1000 most common words\n",
    "dict_size = 1000\n",
    "\n",
    "dictionary = [\"END\", \"START\", \"\\n\"] + transitions_df[\n",
    "    ~transitions_df[\"word\"].str.contains(\" \")\n",
    "].groupby(\"word\").count().sort_values(\"next_word\", ascending=False).index[:dict_size].tolist()\n",
    "\n",
    "# Dedupe in case\n",
    "dictionary = list(dict.fromkeys(dictionary).keys())\n",
    "\n",
    "transitions_df_subset = transitions_df[\n",
    "    transitions_df[\"word\"].str.contains(r'(^| )(' + '|'.join(dictionary) + r')($| )') & \n",
    "    transitions_df[\"next_word\"].isin(dictionary)\n",
    "]\n",
    "\n",
    "print(len(transitions_df))\n",
    "\n",
    "# Compress the words and create mappings to and from the compressed form\n",
    "def compressed_word():\n",
    "    n = 1\n",
    "    while True:\n",
    "        yield from (''.join(group) for group in itertools.product(string.ascii_letters + string.digits, repeat=n))\n",
    "        n += 1\n",
    "        \n",
    "itr = compressed_word()\n",
    "\n",
    "compression_dictionary = {w: next(itr) for w in dictionary}\n",
    "uncompression_dictionary = {v:k for k,v in compression_dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 3957157\n"
     ]
    }
   ],
   "source": [
    "# Create the model - the model is a dictionary mapping words or word pairs to the following words by line number, \n",
    "# ordered by frequency of the following word in the corpus\n",
    "\n",
    "GROUPBYCOL = \"line_num\"\n",
    "model = {}\n",
    "\n",
    "for i, row in transitions_df_subset.groupby([\n",
    "    \"word\", GROUPBYCOL, \"next_word\"\n",
    "]).count().reset_index().sort_values(0, ascending=False).iterrows():\n",
    "    word = row[\"word\"]\n",
    "    next_word = row[\"next_word\"]\n",
    "    \n",
    "    # Handle pairs of words\n",
    "    if \" \" in word:\n",
    "        word1, word2 = word.split(\" \")\n",
    "        \n",
    "        # Make sure that both words are in the dictionary\n",
    "        if (word1 not in dictionary) or (word2 not in dictionary):\n",
    "            continue\n",
    "            \n",
    "        word = compression_dictionary[word1] + \" \" + compression_dictionary[word2]\n",
    "    else:\n",
    "        word = compression_dictionary[word]\n",
    "        \n",
    "    next_word = compression_dictionary[next_word]\n",
    "        \n",
    "    if word not in model:\n",
    "        model[word] = {}\n",
    "    if row[GROUPBYCOL] not in model[word]:\n",
    "        model[word][row[GROUPBYCOL]] = []\n",
    "        \n",
    "    model[word][row[GROUPBYCOL]].append((next_word, row[0]))\n",
    "    \n",
    "\n",
    "# Prune the model to save on space\n",
    "for word in model:\n",
    "    for syllable in model[word]:\n",
    "        total_sum = sum([c for n,c in model[word][syllable]])\n",
    "        cumsum = 0\n",
    "        new_words = []\n",
    "        for next_word, count in model[word][syllable]:\n",
    "            # Exclude options that occur less than .5%\n",
    "            if count < total_sum/250:\n",
    "                continue\n",
    "            \n",
    "            # Exclude infrequent pairs from the space model\n",
    "            if \" \" in word and count < 3:\n",
    "                continue\n",
    "            \n",
    "            if total_sum > 1000:\n",
    "                cumsum += math.ceil(count/1000)\n",
    "            else:\n",
    "                cumsum += count\n",
    "                \n",
    "            new_words.append((next_word, cumsum))\n",
    "            \n",
    "        if new_words:\n",
    "            model[word][syllable] = new_words\n",
    "    \n",
    "    \n",
    "# Get the rough model size; should be ~ 5MB or less\n",
    "import json\n",
    "print(\"Model Size:\", len(json.dumps(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Model Size: 617691\n"
     ]
    }
   ],
   "source": [
    "# Model the ends of haikus in a similar fashion.\n",
    "# There is a separate model for this since we want the haiku to end with a complete thought as best as possible\n",
    "\n",
    "end_model = {}\n",
    "\n",
    "for i, row in transitions_df_subset[\n",
    "    transitions_df_subset[\"end\"] == True\n",
    "].groupby([\"word\", \"next_word\"]).count().reset_index().sort_values(0, ascending=False).iterrows():\n",
    "    word = row[\"word\"]\n",
    "    next_word = row[\"next_word\"]\n",
    "    \n",
    "    # Handle pairs of words\n",
    "    if \" \" in word:\n",
    "        word1, word2 = word.split(\" \")\n",
    "        \n",
    "        # Make sure that both words are in the dictionary\n",
    "        if (word1 not in dictionary) or (word2 not in dictionary):\n",
    "            continue\n",
    "            \n",
    "        word = compression_dictionary[word1] + \" \" + compression_dictionary[word2]\n",
    "    else:\n",
    "        word = compression_dictionary[word]\n",
    "        \n",
    "    next_word = compression_dictionary[next_word]\n",
    "        \n",
    "    if word not in end_model:\n",
    "        end_model[word] = []\n",
    "        \n",
    "    end_model[word].append((next_word, row[0]))\n",
    "    \n",
    "\n",
    "for word in end_model:\n",
    "    total_sum = sum([c for n,c in end_model[word]])\n",
    "    cumsum = 0\n",
    "    new_words = []\n",
    "    for next_word, count in end_model[word]:\n",
    "        # Exclude options that occur less than .1%\n",
    "        if count < total_sum/1000:\n",
    "            continue\n",
    "\n",
    "        cumsum += count\n",
    "\n",
    "        new_words.append((next_word, cumsum))\n",
    "\n",
    "    if new_words:\n",
    "        end_model[word] = new_words\n",
    "    \n",
    "    \n",
    "# Get the rough model size; should be ~ 2.5MB or less\n",
    "import json\n",
    "print(\"End Model Size:\", len(json.dumps(end_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates for writing the Cadence contracts that contain the different parts of the model\n",
    "\n",
    "MODEL_TEMPLATE = \"\"\"pub contract Model {\n",
    "  access(account) let model: {String: {Int: {String: Int}}}\n",
    "\n",
    "  init() {\n",
    "    self.model = {MODEL}\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "SPACE_MODEL_TEMPLATE = \"\"\"pub contract SpaceModel {\n",
    "  access(account) let model: {String: {Int: {String: Int}}}\n",
    "\n",
    "  init() {\n",
    "    self.model = {MODEL}\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "END_MODEL_TEMPLATE = \"\"\"pub contract EndModel {\n",
    "  access(account) let model: {String: {String: Int}}\n",
    "\n",
    "  init() {\n",
    "    self.model = {MODEL}\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "WORDS_TEMPLATE = \"\"\"pub contract Words {\n",
    "  access(account) let syllables: {String: Int}\n",
    "  access(account) let uncompress: {String: String}\n",
    "\n",
    "  init() {\n",
    "    self.syllables = {SYLLABLES}\n",
    "    self.uncompress = {UNCOMPRESS}\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contracts by reformatting the models into Cadence\n",
    "\n",
    "model_reformatted = {}\n",
    "space_model_reformatted = {}\n",
    "\n",
    "output_dir = Path('contracts')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for word, m in model.items():\n",
    "    if \" \" in word:\n",
    "        space_model_reformatted[word.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\")] = {}\n",
    "\n",
    "        for syllable, options in m.items():\n",
    "            # Exclude options where there is no option, to increase diversity\n",
    "            if len(options) < 3:\n",
    "                continue\n",
    "            space_model_reformatted[word.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\")][syllable] = {\n",
    "                w.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\"): c for w, c in options\n",
    "            }\n",
    "            \n",
    "        # If the model for this word turned out empty, delete it\n",
    "        if len(space_model_reformatted[word.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\")]) == 0:\n",
    "            del space_model_reformatted[word.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\")]\n",
    "    else:\n",
    "        model_reformatted[word.replace(\"'\", \"QUOTE\")] = {}\n",
    "        for syllable, options in m.items():\n",
    "            model_reformatted[word.replace(\"'\", \"QUOTE\")][syllable] = {w.replace(\"'\", \"QUOTE\"): c for w, c in options}\n",
    "\n",
    "\n",
    "with (output_dir / 'Model.cdc').open('w') as f:\n",
    "    f.write(MODEL_TEMPLATE.replace(\n",
    "        '{MODEL}',\n",
    "        str(model_reformatted).replace(\"'\", '\"').replace(\"QUOTE\", \"'\").replace(\" \", \"\")\n",
    "    ))\n",
    "\n",
    "with (output_dir / 'SpaceModel.cdc').open('w') as f:\n",
    "    f.write(SPACE_MODEL_TEMPLATE.replace(\n",
    "        '{MODEL}',\n",
    "        str(space_model_reformatted).replace(\"'\", '\"').replace(\"QUOTE\", \"'\").replace(\" \", \"\").replace(\"SPACE\", \" \")\n",
    "    ))\n",
    "\n",
    "\n",
    "end_model_reformatted = {}\n",
    "for word, m in end_model.items():\n",
    "    end_model_reformatted[word.replace(\"'\", \"QUOTE\").replace(\" \", \"SPACE\")] = {w.replace(\"'\", \"QUOTE\"): c for w, c in m}\n",
    "    \n",
    "\n",
    "with (output_dir / 'EndModel.cdc').open('w') as f:\n",
    "    f.write(END_MODEL_TEMPLATE.replace(\n",
    "        '{MODEL}',\n",
    "        str(end_model_reformatted).replace(\"'\", '\"').replace(\"QUOTE\", \"'\").replace(\" \", \"\").replace(\"SPACE\", \" \")\n",
    "    ))\n",
    "    \n",
    "    \n",
    "# Write dictionary contract\n",
    "WORDS[\"END\"] = 0\n",
    "WORDS[\"START\"] = 0\n",
    "words_subset = {w.replace(\"'\", \"QUOTE\"): WORDS[w] for w in dictionary}\n",
    "\n",
    "uncompression_formatted = {k: v.replace(\"'\", \"QUOTE\") for k,v in uncompression_dictionary.items()}\n",
    "\n",
    "with (output_dir / 'Words.cdc').open('w') as f:\n",
    "    f.write(WORDS_TEMPLATE.replace(\n",
    "        '{SYLLABLES}',\n",
    "        str(words_subset).replace(\"'\", '\"').replace(\"QUOTE\", \"'\").replace(\" \", \"\")\n",
    "    ).replace(\n",
    "        '{UNCOMPRESS}',\n",
    "        str(uncompression_formatted).replace(\"'\", '\"').replace(\"QUOTE\", \"'\").replace(\" \", \"\")\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
